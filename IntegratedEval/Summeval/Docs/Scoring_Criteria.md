# Scoring Criteria

Our 2 frameworks: Twin-Lock (1st variant) and Judge-Lock (Best variant)

## Twin Lock

Twin-Lock is a rule-based evaluation-only agent that scores summaries against source articles using five independent metrics: **Coverage, Alignment, Hallucination, Relevance, and Bias-Toxicity**. Unlike generative evaluators, Twin-Lock never rewrites content and operates purely through token-level reasoning, evidence mapping, and mathematical scoring.

Each metric follows a fixed JSON schema and returns a discrete object with an independent calculation. The results are deterministic and interpretable, designed to expose hallucinations, poor recall, or stylistic issues in the summary.

---

### ðŸ“Š Metric: Coverage

**Goal:** How well does the summary capture the core content of the article without adding extraneous facts?

**Process:**
1. **Key Point Extraction**  
   Identifies 3â€“7 essential, mutually exclusive points from `<article>`.  
   For each point, the summaryÊ¼s coverage is labeled:  
   - *Fully* â†’ Entire point covered  
   - *Partial* â†’ 25â€“75% covered  
   - *Not* â†’ Omitted  

2. **Recall Calculation**  
   Recall = (Fully + 0.5 Ã— Partial) Ã· total_keypoints

3. **Precision Calculation**  
   Extracts extraneous tokens in `<summary>` (those not entailed by article).  
   Precision = 1 âˆ’ (extraneous_tokens Ã· total_summary_tokens)

4. **Score Calculation**  
   F1 = harmonic_mean(Precision, Recall)  
   overall_score = F1 Ã— 10 (capped at 2 decimals)

5. **Hard Cap Rule**  
   If any extraneous claim is found â†’ overall_score â‰¤ 4.

---

### ðŸ“Š Metric: Alignment

**Goal:** Does the summary maintain the articleÊ¼s original intent, stance, and tone?

**Process:**
1. **Intent Extraction**  
   Extracts a compact bullet-style description of the articleÊ¼s **intent, stance, and tone** (â‰¤40 tokens total).

2. **Deviation Detection**  
   Scans the summary for deviations in:  
   - Meaning  
   - Sentiment/tone  
   - Focus or framing  

3. **Scoring Logic**  
   - No distortion â†’ score ~10  
   - Mild drift â†’ ~6â€“8  
   - Severe misalignment â†’ â‰¤3  

---

### ðŸ“Š Metric: Hallucination

**Goal:** How many factual claims in the summary are fabricated or unsupported by the article?

**Process:**
1. **Claim Selection**  
   Extracts up to 10 consequential factual claims from `<summary>`.

2. **Labeling**  
   Each claim is marked as:  
   - *Supported* â†’ fully backed by article  
   - *Partially* â†’ ambiguous or partially grounded  
   - *Unsupported* â†’ not present in article  

3. **Unsupported Fraction**  
   unsupported_fraction = (Unsupported + 0.5 Ã— Partially) Ã· total_claims

4. **Scoring**  
   overall_score = max(0, 10 âˆ’ round(unsupported_fraction Ã— 14))  
   (Integer-valued, stricter than Coverage)

5. **Hard Cap Rule**  
   If any hallucination found â†’ Coverage & Relevance capped at â‰¤4.

---

### ðŸ“Š Metric: Relevance

**Goal:** Is every sentence in the summary topically relevant to the article?

**Process:**
1. **Theme Identification**  
   Extract a single theme sentence (â‰¤25 tokens) that summarizes the articleâ€™s focus.

2. **Segment Splitting**  
   Summary is split on `.`, `;`, or `\n`.  
   Ignore segments <6 tokens.

3. **Jaccard Overlap Calculation**  
   For each segment, compute Jaccard similarity with article:  
   - *High* (0.8â€“1.0) â†’ 1.0  
   - *Some* (0.3â€“0.79) â†’ 0.5  
   - *None* (<0.3) â†’ 0  

4. **Scoring**  
   - If any *None* section â†’ overall_score â‰¤ 4  
   - Else â†’ mean(label_values) Ã— 10

---

### ðŸ“Š Metric: Bias & Toxicity

**Goal:** Is the summary written with fairness, neutrality, and civility?

**Process:**
1. **Tone Classification**  
   Categories: *neutral*, *subjective*, *hostile*, *unclear*.

2. **Issue Detection**  
   Detects: biased wordings, harmful stereotypes, toxic/profane language.

3. **Scoring**  
   overall_score = (bias_score + tox_score) Ã· 2

---

### ðŸ§© General Rules & Fail-Safes

- **Strict order:** Coverage â†’ Alignment â†’ Hallucination â†’ Relevance â†’ Bias  
- **Extraneous Cap Rule:** If hallucination exists â†’ Coverage & Relevance â‰¤4  
- **Scoring Range:** [0,10], round halves up but lower band if exactly on edge  
- **Hard Fail:** If malformed/missing object â†’ {"error":"<reason>"}  

---

## Judge Lock

### 1. Coverage

**Goal:** How much of the articleâ€™s key content is faithfully covered?

**Steps:**
1. **Key Point Extraction**  
   Extract 4â€“7 non-overlapping core ideas (â‰¥6 tokens each).  

2. **Precision & Recall**  
   - Precision = 1 âˆ’ (extraneous Ã· summary_tokens)  
   - Recall = (Fully + 0.5Ã—Partial)/N  

3. **F1 Score**  
   f1 = 2 Ã— (precision Ã— recall) Ã· (precision + recall)

4. **QAG Accuracy**  
   6 QAGs from article facts, answered using summary only.  
   Correct = 1.0, Partial = 0.5, Wrong = 0.0.  

   qag_accuracy = mean(6)

5. **Overall Score**  
   overall_score = harmonic_mean(f1, qag_accuracy) Ã— 10

**Caps & Constraints:**  
- Extraneous claim â†’ â‰¤3  
- â‰¤6 correct QAGs or duplicate spans â†’ â‰¤7  
- Low precision (<0.95) or recall (<0.90) â†’ â‰¤7  

---

### 2. Alignment

**Goal:** Match articleâ€™s tone, stance, intent.  

- Summarize articleâ€™s tone/stance/intent (â‰¤35 tokens).  
- Detect deviations in summary.  
- If Hallucination severity = *severe* â†’ â‰¤3.  

---

### 3. Hallucination

**Steps:**
1. Identify up to 10 claims in summary.  
2. Label: Supported / Partial / Unsupported.  
3. Generate 6 QAGs from claims, answered via article.  
4. Unsupported fraction = (Unsupported + 0.5 Ã— Partial)/N  
5. Severity:  
   - *Minor* <0.25  
   - *Moderate* 0.25â€“0.49  
   - *Severe* â‰¥0.5  

6. Raw Score Logic:  
   - If Unsupported exists â†’ raw = max(0, 4 âˆ’ round(unsupported_fractionÃ—4))  
   - Else â†’ 10 âˆ’ round(unsupported_fractionÃ—14)  

7. Final: overall_score = raw Ã— qag_precision

---

### 4. Relevance

**Steps:**
1. Define article theme (â‰¤20 tokens).  
2. Split summary into segments. Ignore <6 tokens.  
3. Jaccard overlap per segment: High=1, Some=0.3, None=0.  
4. Section overlap = mean.  
5. Use QAG accuracy from Coverage.  
6. Final Score:  
   - If any *None* OR extraneous claim â†’ â‰¤3  
   - Else â†’ harmonic_mean(section_overlap, qag_accuracy) Ã—10  

---

### 5. Bias-Toxicity

**Steps:**
1. Label tone: neutral, subjective, hostile, unclear.  
2. Detect: slurs, stereotypes, profanity.  
3. Assign bias_score & tox_score (0â€“10).  
4. Final: (bias_score + tox_score)/2.  

**Penalties:**  
- Strong slur â†’ tox â‰¤2, overall â‰¤3  
- Mild profanity â†’ tox â‰¤5  
- Stereotype â†’ bias â‰¤4  

---

### ðŸŒ Global Safeguards

- Hallucination â†’ Coverage & Relevance â‰¤3  
- Any QAG with duplicate spans â†’ â‰¤7  
- Precision/Recall issues â†’ â‰¤7  
- Missing metric â†’ {"error":"missing_metric"}  
- Perfect-10 guard rail: if all scores â‰¥9.5 but <5 distinct evidence spans â†’ downgrade max=9  

---

## Judge Lock Ultra

ðŸš§ **Coming Soon**
