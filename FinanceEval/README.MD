# FinanceEval ðŸ’° 

## What is FinanceEval?

FinanceEval assesses the quality of financial advice given by a model to a human across six metrics: Trust & Transparency, Competence & Accuracy, Explainability, Client-Centeredness/Fiduciary Alignment, Bias-Sensitivity & Risk Safety, and Financial Literacy Support/Communication Clarity. It combines deterministic rules, math, and LLM/NLP hybrid evaluation to produce per-metric scores (1â€“5), explanations, and a JSON output.

## Hugging Face Space

FinanceEval Space

## What Does It Do?

- Evaluates financial advice across six research-backed metrics.
- Provides detailed scores (1â€“10) with evidence and reasoning.
- Generates a structured JSON output for each evaluation run.
- Supports analysis across sub-domains like Budgeting, Investments, and Retirement Planning.

## How It Works

- **Input:** Plain text financial advice transcript.
- **Process:** Select evaluation sub-domain, run with default or custom weights.
- **Output:** JSON with metric scores, flags, evidence, and reasoning.

ðŸ‘‰ Try it live: [FinanceEval on Hugging Face Spaces](https://huggingface.co/spaces/BrainDrive/FinanceEval)

## Scoring Criteria (High-Level)

- **Trust & Transparency:** Assesses honesty, risk disclosures, and balanced tone.
- **Competence & Accuracy:** Checks numeric accuracy and semantic plausibility.
- **Explainability:** Evaluates clarity and causal reasoning in advice.
- **Client-Centeredness/Fiduciary Alignment:** Measures personalization and conflict handling.
- **Bias-Sensitivity & Risk Safety:** Detects biases and ensures risk disclosures.
- **Financial Literacy Support/Communication Clarity:** Gauges jargon use and teaching clarity.

## Features âœ¨

- Research-grounded metric design.
- Hybrid LLM/NLP evaluation framework.
- Supports multiple finance sub-domains.
- JSON output for detailed analysis.

## Installation ðŸ”§

```bash
cd FinanceEval
pip install -r requirements.txt
```

---

## Environment Variables (API Keys)

FinanceEval can use LLM assistance from **OpenAI** or **Anthropic**.  
Set your keys before running the app:

### macOS / Linux (bash/zsh)

```bash
export OPENAI_API_KEY="your_openai_key_here"
export ANTHROPIC_API_KEY="your_anthropic_key_here"
```

### Windows (PowerShell)

```powershell
setx OPENAI_API_KEY "your_openai_key_here"
setx ANTHROPIC_API_KEY "your_anthropic_key_here"
```


## Run Locally

```bash
python3 app.py
```

The app starts on [http://localhost:7860](http://localhost:7860) by default.

---

## Project Structure

```
financeeval/
â”œâ”€â”€ app.py            # Gradio app entrypoint
â”œâ”€â”€ core/             # Core evaluation logic
â”‚   â”œâ”€â”€ providers.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ evaluators.py
â”‚   â”œâ”€â”€ fusion.py
â”‚   â””â”€â”€ schema.py
â”œâ”€â”€ nlp/              # NLP modules
â”‚   â”œâ”€â”€ detectors.py
â”‚   â”œâ”€â”€ trust.py
â”‚   â”œâ”€â”€ accuracy.py
â”‚   â”œâ”€â”€ explain.py
â”‚   â”œâ”€â”€ client_first.py
â”‚   â”œâ”€â”€ risk_safety.py
â”‚   â””â”€â”€ clarity.py
â”œâ”€â”€ prompts/          # LLM prompts
â”‚   â”œâ”€â”€ trust.txt
â”‚   â”œâ”€â”€ accuracy.txt
â”‚   â”œâ”€â”€ explain.txt
â”‚   â”œâ”€â”€ client_first.txt
â”‚   â”œâ”€â”€ risk_safety.txt
â”‚   â””â”€â”€ clarity.txt
â”œâ”€â”€ schema/           # Output schema
â”‚   â””â”€â”€ output_schema.json
â”œâ”€â”€ tests/            # Test data
â”‚   â”œâ”€â”€ redteam_inputs.jsonl
â”‚   â””â”€â”€ golden_outputs.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Privacy & Keys

Inputs are processed in-memory. LLM usage requires API keys set via environment variables or .env file. Remove keys before sharing.

## Troubleshooting

- Missing output: Check LLM API connectivity.
- Low scores: Review metric definitions in the document.

## Outputs

- JSON with metric scores and reasoning.
- Detailed evaluation per metric.

## Research and Documentation

For detailed knowledge about the FinanceEval evaluation framework, metric design, research grounding, and reporting outputs, please refer to the Docs folder:

ðŸ‘‰ [FinanceEval Docs on GitHub](https://github.com/BrainDriveAI/ModelMatch/tree/main/FinanceEval/Docs)

The Docs include:
- **Metrics.md** â€” exact scoring math, rules, and guardrails for each metric.  
- **Research.md** â€” synthesis of academic and industry sources applied metric-wise.  
- **Results.md** â€” output schema, JSON/CSV formats, and leaderboard reporting examples.  



