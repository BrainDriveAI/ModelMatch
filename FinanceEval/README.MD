# FinanceEval ðŸ’° 

## What is FinanceEval?

FinanceEval assesses the quality of financial advice given by a model to a human across six metrics: Trust & Transparency, Competence & Accuracy, Explainability, Client-Centeredness/Fiduciary Alignment, Bias-Sensitivity & Risk Safety, and Financial Literacy Support/Communication Clarity. It combines deterministic rules, math, and LLM/NLP hybrid evaluation to produce per-metric scores (1â€“5), explanations, and a JSON output.

## Hugging Face Space

FinanceEval Space

## What Does It Do?

- Evaluates financial advice across six research-backed metrics.
- Provides detailed scores (1â€“10) with evidence and reasoning.
- Generates a structured JSON output for each evaluation run.
- Supports analysis across sub-domains like Budgeting, Investments, and Retirement Planning.

## How It Works

- **Input:** Plain text financial advice transcript.
- **Process:** Select evaluation sub-domain, run with default or custom weights.
- **Output:** JSON with metric scores, flags, evidence, and reasoning.

ðŸ‘‰ Try it live: [FinanceEval on Hugging Face Spaces](https://huggingface.co/spaces/BrainDrive/FinanceEval)

## Scoring Criteria (High-Level)

- **Trust & Transparency:** Assesses honesty, risk disclosures, and balanced tone.
- **Competence & Accuracy:** Checks numeric accuracy and semantic plausibility.
- **Explainability:** Evaluates clarity and causal reasoning in advice.
- **Client-Centeredness/Fiduciary Alignment:** Measures personalization and conflict handling.
- **Bias-Sensitivity & Risk Safety:** Detects biases and ensures risk disclosures.
- **Financial Literacy Support/Communication Clarity:** Gauges jargon use and teaching clarity.

## Features âœ¨

- Research-grounded metric design.
- Hybrid LLM/NLP evaluation framework.
- Supports multiple finance sub-domains.
- JSON output for detailed analysis.

## Installation ðŸ”§

```bash
cd FinanceEval
pip install -r requirements.txt
```

## Run Locally

```bash
python3 app.py
```

The app starts on http://localhost:7860 by default.

## Project Structure

```
financeeval/
â”œâ”€â”€ app.py            # Gradio app entrypoint
â”œâ”€â”€ core/             # Core evaluation logic
â”‚   â”œâ”€â”€ providers.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ evaluators.py
â”‚   â”œâ”€â”€ fusion.py
â”‚   â””â”€â”€ schema.py
â”œâ”€â”€ nlp/              # NLP modules
â”‚   â”œâ”€â”€ detectors.py
â”‚   â”œâ”€â”€ trust.py
â”‚   â”œâ”€â”€ accuracy.py
â”‚   â”œâ”€â”€ explain.py
â”‚   â”œâ”€â”€ client_first.py
â”‚   â”œâ”€â”€ risk_safety.py
â”‚   â””â”€â”€ clarity.py
â”œâ”€â”€ prompts/          # LLM prompts
â”‚   â”œâ”€â”€ trust.txt
â”‚   â”œâ”€â”€ accuracy.txt
â”‚   â”œâ”€â”€ explain.txt
â”‚   â”œâ”€â”€ client_first.txt
â”‚   â”œâ”€â”€ risk_safety.txt
â”‚   â””â”€â”€ clarity.txt
â”œâ”€â”€ schema/           # Output schema
â”‚   â””â”€â”€ output_schema.json
â”œâ”€â”€ tests/            # Test data
â”‚   â”œâ”€â”€ redteam_inputs.jsonl
â”‚   â””â”€â”€ golden_outputs.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Privacy & Keys

Inputs are processed in-memory. LLM usage requires API keys set via environment variables or .env file. Remove keys before sharing.

## Troubleshooting

- Missing output: Check LLM API connectivity.
- Low scores: Review metric definitions in the document.

## Contributing

Open issues for bugs or enhancements. Update documentation and cite sources when modifying metrics.

## Outputs

- JSON with metric scores, flags, evidence, and reasoning.
- Detailed evaluation per sub-domain.

## Research References

Metrics are based on findings from papers. See the document for full references.
